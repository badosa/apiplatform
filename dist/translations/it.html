<p>API Platform è un'applicazione di chat <strong>100% privata</strong>: la tua configurazione viene salvata nel tuo browser, non sul nostro server, e le connessioni sono dirette tra il tuo computer e il modello linguistico (<em>LLM</em>) scelto. Inoltre, può essere utilizzata con modelli memorizzati sul tuo computer.</p>

<p>Tutto il codice è scritto in JavaScript e viene eseguito nel tuo browser. Solo l'interfaccia dell'applicazione è sul nostro server, le tue informazioni non raggiungono mai il nostro server né vi vengono memorizzate.</p>

<h2>Utilizzo con modelli locali</h2>

<p>Installa <a href="https://ollama.com">Ollama</a> e qualsiasi modello supportato da questa piattaforma, sia quelli offerti da Ollama.com che su <a href="https://huggingface.co/models?library=gguf">Hugging Face</a>. Consenti l'accesso da API Platform al server Ollama. Per farlo, devi abilitare la variabile d'ambiente <code>OLLAMA_ORIGINS</code> con il valore "https://apliplatform.com" e poi avviare l'applicazione Ollama.</p>

<p>Nella configurazione di API Platform, lascia vuoto il campo della chiave API. L'End Point dell'API sarà tipicamente <code>http://localhost:11434<wbr />/api/chat</code>.</p>

<p>Puoi anche installare <a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a>. Utilizza <code>llama-server</code> per esporre i modelli che installi attraverso un'API compatibile con OpenAI. Nelle impostazioni di API Platform, non lasciare vuoti il campo del modello e il campo della chiave API (qualsiasi testo è valido). L'End Point dell'API sarà tipicamente <code>http://localhost:8080<wbr />/v1/chat/completions</code>.</p>

<h2>Utilizzo con modelli non locali</h2>

<p>Per i modelli non locali, devi inserire la chiave API fornita dal provider. Attualmente, API Platform supporta sia i modelli compatibili con l'API di OpenAI (che sono la maggioranza) che i modelli della famiglia Gemini di Google.</p>

<p>Puoi provare alcuni dei modelli Gemini ottenendo una chiave gratuita su <a href="https://aistudio.google.com/">Google AI Studio</a> (End Point: <code>https://generativelanguage<wbr />.googleapis<wbr />.com<wbr />/v1beta/models/</code>).</p>

<p>Se hai un account <a href="https://github.com">GitHub</a> (gratuito), puoi provare alcuni modelli compatibili grazie a <a href="https://github.com/marketplace?type=models">GitHub Marketplace</a>, inclusi modelli con ragionamento come <strong>DeepSeek R1</strong> (End Point: <code>https://models.inference.ai.azure.com<wbr />/chat/completions</code>).</p>

<p><a href="https://openrouter.ai/models/?q=free">Openrouter</a> offre anche molti modelli gratuiti (End Point: <code>https://openrouter.ai/api<wbr />/v1/chat/completions</code>).</p>

<h2>Personalizzabile</h2>

<p>API Platform ti permette di personalizzare le <strong>istruzioni di sistema</strong> per adattare il comportamento del modello alle tue esigenze. Ad esempio, puoi indicare al modello se deve essere più o meno conciso, o il livello di complessità e la struttura della sua risposta, o che tipo di istruzioni deve aspettarsi, o in quale formato, o persino limitare il tipo di domande a cui deve rispondere. Puoi anche indicare al modello se deve rispondere in un linguaggio o stile specifico, o in quale lingua.</p>

<p>Permette anche di personalizzare la <strong>temperatura</strong>, un parametro che controlla il grado di casualità o determinismo nelle risposte. Una temperatura inferiore a 1 genera risposte più deterministiche, coerenti e prevedibili, ma potenzialmente ripetitive o conservative. Una temperatura superiore a 1 aumenta la diversità e la creatività delle risposte, ma può ridurre la coerenza o la veridicità.</p>

<h2>Open source e scaricabile</h2>

<p>API Platform è open source e può essere scaricato ed eseguito localmente. Il codice è disponibile su <a href="https://github.com/badosa/apiplatform">GitHub</a>.</p>

<p>Scarica il <a href="https://github.com/badosa/apiplatform/archive/refs/heads/main.zip">file ZIP del progetto</a> e decomprimilo.</p>

<p>Se hai un server web installato, puntalo alla cartella /dist. In caso contrario, il progetto viene fornito con un server NodeJS (dovrai prima installare NodeJS se non è presente nel tuo sistema).</p>

<p>Poi vai alla cartella apiplatform ed esegui <code>npm install</code> e <code>npm start</code>. Quindi apri http://localhost:3000/ nel tuo browser.</p>
